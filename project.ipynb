{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd921bc",
   "metadata": {},
   "source": [
    "# Multi-Task Learning for Food Ingredient Classification and Segmentation \n",
    "### Deep Learning for Visual Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c2a70",
   "metadata": {},
   "source": [
    "### 1. Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41624fdb",
   "metadata": {},
   "source": [
    "#### 1.1 Load Dataset and Metadata\n",
    "This section loads the `FoodSeg103` dataset and associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch, torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import list_models, get_model\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "ds = load_dataset(\"EduardoPacheco/FoodSeg103\")\n",
    "\n",
    "with open(\"data/idtolabel.json\") as f:\n",
    "    id2label = json.load(f)\n",
    "\n",
    "num_classes = len(id2label)\n",
    "label_to_idx = {v:k for k,v in id2label.items()}\n",
    "label_list = list(label_to_idx.keys())\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"NUmber of train images: {len(ds['train'])}\")\n",
    "print(f\"NUmber of validation images: {len(ds['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db51762",
   "metadata": {},
   "source": [
    "#### 1.2 Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples:\")\n",
    "df_examples = pd.DataFrame(ds['train'][:5])\n",
    "df_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f96e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 3\n",
    "fig, axes = plt.subplots(2, n_examples, figsize=(4 * n_examples, 8))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    example = ds[\"train\"][i]\n",
    "    img = example[\"image\"]\n",
    "    mask = np.array(example[\"label\"])\n",
    "    class_ids = example[\"classes_on_image\"]\n",
    "    class_names = [label_list[c] for c in class_ids]\n",
    "\n",
    "    # IMAGE\n",
    "    axes[0, i].imshow(img)\n",
    "    # Show class names as a list, one per line, to avoid overlap\n",
    "    axes[0, i].set_title(f\"Classes:\\n{class_names}\", fontsize=10)\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    # MASK\n",
    "    axes[1, i].imshow(mask, cmap=\"jet\", alpha=0.8)\n",
    "    axes[1, i].set_title(f\"Mask\\nShape: {mask.shape}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9b4d0",
   "metadata": {},
   "source": [
    "#### 1.3 Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a173a",
   "metadata": {},
   "source": [
    "##### 1.3.1 Distribution Across Training Set\n",
    "The dataset exhibits severe class imbalance, with many classes appearing in fewer than 50 images. This is expected to affect the classification branch significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class distribution in the training set\n",
    "\n",
    "all_classes = []\n",
    "for example in ds[\"train\"]:\n",
    "    all_classes.extend(example[\"classes_on_image\"])\n",
    "\n",
    "class_counts = Counter(all_classes)\n",
    "class_names_ordered = [label_list[class_id] for class_id in range(len(label_list))]\n",
    "counts_ordered = [class_counts.get(class_id, 0) for class_id in range(len(label_list))]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.barplot(x=class_names_ordered, y=counts_ordered)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Food Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a41ee3",
   "metadata": {},
   "source": [
    "##### 1.3.2 Top 10 Most Frequent Ingredients\n",
    "The bar plot below shows the top 10 most frequent ingredient classes in the training set (excluding background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = [item for item in class_counts.most_common() if item[0] != 0][:10]\n",
    "top10_ids, top10_counts = zip(*top10)\n",
    "top10_labels = [label_list[c] for c in top10_ids]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=top10_labels, y=top10_counts)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Top 10 Most Frequent Classes (Excluding Background)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451f309",
   "metadata": {},
   "source": [
    "##### 1.3.3 The Most Frequent Class\n",
    "The most frequent class in the training set is **bread** (ID: 58), appearing in 991 images. We will use this class to create a smaller dataset for efficient training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class counts\n",
    "class_counts = Counter(all_classes)\n",
    "\n",
    "# Get the most common class (excluding background, which is class 0)\n",
    "top_class_id, top_class_count = next((cls_id, count) for cls_id, count in class_counts.most_common() if cls_id != 0)\n",
    "top_class_name = label_list[top_class_id]\n",
    "print(f\"Most common class (excluding background): {top_class_name} (ID: {top_class_id}) - Count: {top_class_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4909838",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fc64f",
   "metadata": {},
   "source": [
    "#### 2.1 Filter Dataset\n",
    "This step filters the dataset to retain only images containing the most frequent class (**bread**, ID: 58). This allows focused analysis and model training on a subset with sufficient samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a530b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filtering to ds to keep only images containing the top class\n",
    "def filter_top_class(example):\n",
    "    return top_class_id in example[\"classes_on_image\"]\n",
    "\n",
    "ds = ds.filter(filter_top_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a42d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate num_classes after filtering\n",
    "filtered_classes = set()\n",
    "for example in ds[\"train\"]:\n",
    "    if \"classes_on_image\" in example:\n",
    "        filtered_classes.update(example[\"classes_on_image\"])\n",
    "num_classes = len(filtered_classes)\n",
    "print(f\"Updated num_classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d18eb0",
   "metadata": {},
   "source": [
    "#### 2.2 Label Processing\n",
    "To enable multi-label classification, each image's ingredient labels are converted into a one-hot encoded vector. This vector indicates the presence or absence of each ingredient class in the image, making it suitable for training classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc062497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(example):\n",
    "    ingredient_vector = torch.zeros(num_classes, dtype=torch.float32)\n",
    "    if \"classes_on_image\" in example:\n",
    "        for cls in example[\"classes_on_image\"]:\n",
    "            if 0 <= cls < num_classes:\n",
    "                ingredient_vector[cls] = 1.0\n",
    "    example[\"ingredient_vector\"] = ingredient_vector\n",
    "    return example\n",
    "\n",
    "ds = ds.map(one_hot_encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e43c1",
   "metadata": {},
   "source": [
    "#### 2.3 Image and Mask Transforms\n",
    "All images and segmentation masks are resized to a fixed size of 256x256. This ensures consistent input dimensions for both classification and segmentation models. Images are normalized to match ImageNet statistics, and masks are converted to tensors with nearest-neighbor interpolation to preserve label integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define torchvision transforms for resizing and tensor conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "def resize_and_to_tensor(example):\n",
    "    example['image'] = transform(example['image'])\n",
    "    example['label'] = label_transform(example['label']).long().squeeze(0)\n",
    "\n",
    "    return example\n",
    "\n",
    "# Apply resizing and tensor conversion to all splits\n",
    "ds = ds.map(resize_and_to_tensor)\n",
    "ds.set_format(\"torch\", columns=[\"image\", \"label\", \"ingredient_vector\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850ec35",
   "metadata": {},
   "source": [
    "#### 2.4 Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = ds[\"train\"], ds[\"validation\"]\n",
    "train, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584fd37",
   "metadata": {},
   "source": [
    "#### 2.5 DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train, batch_size=8, shuffle=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation, batch_size=8, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "train_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de366dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab2807",
   "metadata": {},
   "source": [
    "### 3. Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d2256",
   "metadata": {},
   "source": [
    "#### 3.1 Model Architecture\n",
    "The classification model uses a `ResNet-50` backbone pretrained on `ImageNet`. All layers are frozen except the final fully connected (fc) layer, which is replaced with a new linear layer to output logits for each ingredient class (`num_classes`). \n",
    "\n",
    "This enables multi-label classification, where the model predicts the presence or absence of each ingredient in an image. \n",
    "\n",
    "The model outputs a vector of logits, one per class, which are passed through a sigmoid activation during evaluation to obtain probabilities for each ingredient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer to match the number of ingredient classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e5d02",
   "metadata": {},
   "source": [
    "#### 3.2 Loss Function, Optimizer, and Learning Rate Scheduler\n",
    "\n",
    "For multi-label ingredient classification, the `BCEWithLogitsLoss` is used, which combines a sigmoid layer and the binary cross-entropy loss in a single class. \n",
    "\n",
    "The optimizer is `Adam`, which adapts the learning rate for each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc60811",
   "metadata": {},
   "source": [
    "#### 3.3 Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "num_epochs = 10  \n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "subset_accs = []\n",
    "print(\"Starting training the classification model\")\n",
    "for epoch in range(num_epochs):\n",
    "    # -----------------------------\n",
    "    # TRAINING\n",
    "    # -----------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] - Training\", leave=False)\n",
    "    for batch in loop:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"ingredient_vector\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = running_loss / len(train)\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATION\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] - Validation\", leave=False):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"ingredient_vector\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Collect for metrics\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(validation)\n",
    "    \n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    preds_np = (all_outputs > 0.5).astype(np.float32)\n",
    "\n",
    "    precisions.append(precision_score(all_targets, preds_np, average='macro', zero_division=0))\n",
    "    recalls.append(recall_score(all_targets, preds_np, average='macro', zero_division=0))\n",
    "    f1_scores.append(f1_score(all_targets, preds_np, average='macro', zero_division=0))\n",
    "    subset_accs.append(np.mean(np.all(preds_np == all_targets, axis=1)))\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"models/resnet50_final2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e70fe",
   "metadata": {},
   "source": [
    "#### 3.3 Evaluation Metrics\n",
    "The classification model is evaluated using several metrics:\n",
    "\n",
    "- **Precision**: Measures the proportion of predicted ingredient labels that are actually present in the image. Low precision indicates many false positives.\n",
    "\n",
    "- **Recall**: Measures the proportion of true ingredient labels that are correctly predicted by the model. Low recall indicates many missed ingredients.\n",
    "\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a balanced measure of accuracy for multi-label classification.\n",
    "\n",
    "- **Subset Accuracy**: The percentage of images for which all ingredient labels are predicted exactly. This is a strict metric and typically low for multi-label tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, precisions, label=\"Precision\")\n",
    "plt.plot(epochs, recalls, label=\"Recall\")\n",
    "plt.plot(epochs, f1_scores, label=\"F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, subset_accs, label=\"Subset Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Subset Accuracy\")\n",
    "plt.title(\"Subset Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0753a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Precision:  {np.mean(precisions):.4f}\")\n",
    "print(f\"Mean Recall:     {np.mean(recalls):.4f}\")\n",
    "print(f\"Mean F1:         {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean Subset Acc: {np.mean(subset_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c714e7b",
   "metadata": {},
   "source": [
    "**Results Summary:**\n",
    "\n",
    "The classification model demonstrates limited ability to accurately identify all ingredient classes in multi-label food images. \n",
    "\n",
    "Precision is low, indicating a tendency to predict ingredients that are not present (false positives). \n",
    "Recall is also low, showing that many true ingredients are missed by the model. \n",
    "The F1 score, which balances precision and recall, remains modest, reflecting overall challenges in multi-label classification. \n",
    "Subset accuracy is very low, meaning the model rarely predicts all ingredients in an image exactly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3662ba",
   "metadata": {},
   "source": [
    "#### 3.4 Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_display = 6\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "shown = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        targets = batch[\"ingredient_vector\"]\n",
    "\n",
    "        outputs = model(images)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        for j in range(len(images)):\n",
    "            if shown >= num_display:\n",
    "                break\n",
    "\n",
    "            ax = axes[shown]\n",
    "            img_np = images[j].cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = (img_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "            img_np = img_np.clip(0, 1)\n",
    "            ax.imshow(img_np)\n",
    "\n",
    "            true_idxs = np.where(targets[j].cpu().numpy() == 1)[0]\n",
    "            pred_idxs = np.where(preds[j] == 1)[0]\n",
    "            true_labels = [label_list[k] for k in true_idxs]\n",
    "            pred_labels = [label_list[k] for k in pred_idxs]\n",
    "\n",
    "            ax.set_title(f\"True: {true_labels}\\nPred: {pred_labels}\", fontsize=8)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            shown += 1\n",
    "\n",
    "        if shown >= num_display:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08552a7f",
   "metadata": {},
   "source": [
    "### 4. Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d6735",
   "metadata": {},
   "source": [
    "#### 4.1 Model Architecture\n",
    "The segmentation model uses `DeepLabV3` with a `ResNet-50` backbone, a popular architecture for semantic segmentation. The backbone extracts hierarchical image features, while the `DeepLabV3` head applies *Atrous Spatial Pyramid Pooling* (ASPP) to capture multi-scale context. \n",
    "\n",
    "The final classifier layer is replaced to output `num_classes` channels, matching the number of ingredient classes in the dataset. This enables pixel-wise classification for each ingredient class in the input image. The model is initialized with pretrained weights for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36359e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\"deeplabv3_resnet50\", weights=\"DEFAULT\")\n",
    "model.classifier[-1] = nn.Conv2d(model.classifier[-1].in_channels, num_classes, kernel_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb34f76",
   "metadata": {},
   "source": [
    "#### 4.2 Loss Functions and Evaluation Metrics\n",
    "\n",
    "This section defines the loss functions and evaluation metrics used for training and validating the segmentation model. The primary loss is `CrossEntropyLoss`, suitable for multi-class pixel-wise segmentation. \n",
    "\n",
    "Evaluation metrics include **pixel accuracy**, **per-class Intersection over Union** (IoU), **mean IoU**, and **Dice score**, providing a comprehensive assessment of segmentation performance across all ingredient classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def pixel_accuracy(outputs, labels):\n",
    "    outputs = outputs.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    correct = (preds == labels).float()\n",
    "    acc = correct.sum() / correct.numel()\n",
    "    return acc.item()\n",
    "\n",
    "def per_class_iou(outputs, labels, num_classes):\n",
    "    outputs = outputs.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return ious\n",
    "\n",
    "def mean_iou(outputs, labels, num_classes):\n",
    "    outputs = outputs.cpu()\n",
    "    labels = labels.cpu()\n",
    "    ious = per_class_iou(outputs, labels, num_classes)\n",
    "    valid_ious = [iou for iou in ious if not np.isnan(iou)]\n",
    "    return np.mean(valid_ious)\n",
    "\n",
    "def dice_score(outputs, labels, num_classes):\n",
    "    outputs = outputs.cpu()\n",
    "    labels = labels.cpu()\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    dice_scores = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (labels == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        pred_sum = pred_inds.sum().item()\n",
    "        target_sum = target_inds.sum().item()\n",
    "        if pred_sum + target_sum == 0:\n",
    "            dice_scores.append(float('nan'))\n",
    "        else:\n",
    "            dice_scores.append(2 * intersection / (pred_sum + target_sum))\n",
    "    return dice_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df814e",
   "metadata": {},
   "source": [
    "#### 4.3 Training Loop\n",
    "The training loop for the segmentation model iterates over a fixed number of epochs, alternating between training and validation phases. In each training epoch, the model processes batches of images and segmentation masks, computes the cross-entropy loss, and updates the model weights using backpropagation. \n",
    "\n",
    "After each epoch, the model is evaluated on the validation set, where losses and segmentation metrics are computed and tracked. The training and validation losses, as well as the metrics, are stored for later visualization. \n",
    "\n",
    "The best model is saved at the end of training. This process enables monitoring of both model convergence and segmentation performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# metrics\n",
    "pixel_accs = []\n",
    "ious_list = []\n",
    "dice_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -----------------------------\n",
    "    # TRAINING\n",
    "    # -----------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device).squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)[\"out\"]\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATION\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    epoch_pixel_acc = []\n",
    "    epoch_ious = []\n",
    "    epoch_dices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device).squeeze(1)\n",
    "\n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # metrics per batch\n",
    "            epoch_pixel_acc.append(pixel_accuracy(outputs, labels))\n",
    "            epoch_ious.append(per_class_iou(outputs, labels, num_classes))\n",
    "            epoch_dices.append(dice_score(outputs, labels, num_classes))\n",
    "\n",
    "    # compute per-epoch means\n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    pixel_accs.append(np.mean(epoch_pixel_acc))\n",
    "    ious_list.append(np.nanmean(np.array(epoch_ious), axis=0))\n",
    "    dice_list.append(np.nanmean(np.array(epoch_dices), axis=0))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"models/deeplabv3_resnet50_final2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f8e80",
   "metadata": {},
   "source": [
    "#### 4.4 Evaluation Metrics\n",
    "The segmentation model is evaluated using several metrics:\n",
    "\n",
    "- **Pixel Accuracy**: Measures the proportion of correctly classified pixels over all pixels. High pixel accuracy indicates that most pixels are assigned the correct class, but it can be biased by dominant classes.\n",
    "\n",
    "- **Mean IoU (Intersection over Union)**: Computes the average IoU across all classes, where IoU for each class is the ratio of the intersection to the union of predicted and ground truth pixels. This metric is robust to class imbalance and reflects how well the model segments each class.\n",
    "\n",
    "- **Mean Dice Score**: The Dice coefficient is another overlap metric, similar to IoU, but gives more weight to correctly predicted pixels. The mean Dice score averages this value across all classes, providing a balanced view of segmentation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c56c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel_acc = np.mean(pixel_accs)\n",
    "mean_ious = np.nanmean(np.array(ious_list), axis=0)\n",
    "mean_iou_score = np.nanmean(mean_ious)\n",
    "mean_dice = np.nanmean(np.array(dice_list), axis=0)\n",
    "mean_dice_score = np.nanmean(mean_dice)\n",
    "\n",
    "print(f\"Pixel Accuracy: {mean_pixel_acc:.4f}\")\n",
    "print(f\"Mean IoU: {mean_iou_score:.4f}\")\n",
    "print(f\"Mean Dice Score: {mean_dice_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086d8e4",
   "metadata": {},
   "source": [
    "**Results Summary:** \n",
    "\n",
    "- **Pixel Accuracy:** The model correctly classifies a big percentage of all pixels, indicating good performance on dominant classes (background).\n",
    "- **Mean IoU:** The low mean Intersection over Union shows poor overlap between predicted and true masks, especially for minority classes.\n",
    "- **Mean Dice Score:** The low mean Dice score further confirms that the model struggles to segment less frequent ingredient classes.\n",
    "\n",
    "Overall, while the model achieves high pixel accuracy by favoring common classes like **background**, its ability to segment all ingredient classes (especially rare ones) remains limited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcece98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ---- Loss Curves ----\n",
    "axs[0, 0].plot(train_losses, label=\"Train Loss\")\n",
    "axs[0, 0].plot(val_losses, label=\"Validation Loss\")\n",
    "axs[0, 0].set_xlabel(\"Epoch\")\n",
    "axs[0, 0].set_ylabel(\"Loss\")\n",
    "axs[0, 0].set_title(\"Training vs Validation Loss\")\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].grid(True)\n",
    "\n",
    "# ---- Pixel Accuracy ----\n",
    "axs[0, 1].plot(pixel_accs, label=\"Pixel Accuracy\", color='tab:orange')\n",
    "axs[0, 1].set_xlabel(\"Epoch\")\n",
    "axs[0, 1].set_ylabel(\"Accuracy\")\n",
    "axs[0, 1].set_title(\"Pixel Accuracy Over Epochs\")\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# ---- Mean IoU ----\n",
    "mean_ious = [np.nanmean(iou) for iou in ious_list]\n",
    "axs[1, 0].plot(mean_ious, label=\"Mean IoU\", color='tab:green')\n",
    "axs[1, 0].set_xlabel(\"Epoch\")\n",
    "axs[1, 0].set_ylabel(\"IoU\")\n",
    "axs[1, 0].set_title(\"Mean IoU Over Epochs\")\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# ---- Mean Dice ----\n",
    "mean_dices = [np.nanmean(dice) for dice in dice_list]\n",
    "axs[1, 1].plot(mean_dices, label=\"Mean Dice\", color='tab:red')\n",
    "axs[1, 1].set_xlabel(\"Epoch\")\n",
    "axs[1, 1].set_ylabel(\"Dice Score\")\n",
    "axs[1, 1].set_title(\"Mean Dice Over Epochs\")\n",
    "axs[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean IoU per class across all epochs\n",
    "mean_ious_per_class = np.nanmean(np.array(ious_list), axis=0)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(range(num_classes), mean_ious_per_class)\n",
    "filtered_label_list = [label_list[c] for c in sorted(filtered_classes)]\n",
    "plt.xticks(range(len(filtered_label_list)), filtered_label_list, rotation=90)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Mean IoU\")\n",
    "plt.title(\"Mean IoU per Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201430d6",
   "metadata": {},
   "source": [
    "#### 4.5 Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eff219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from validation loader\n",
    "batch = next(iter(val_loader))\n",
    "images = batch[\"image\"].to(device)\n",
    "labels = batch[\"label\"].cpu().numpy()  # (N, H, W)\n",
    "\n",
    "# Get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)[\"out\"]\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()  # (N, H, W)\n",
    "\n",
    "# Show for the first 3 images in the batch\n",
    "num_show = 3\n",
    "for i in range(num_show):\n",
    "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # unnormalize\n",
    "    img = img.clip(0, 1)\n",
    "\n",
    "    gt_mask = labels[i]\n",
    "    pred_mask = preds[i]\n",
    "\n",
    "    # Overlay predicted mask on image\n",
    "    overlay = img.copy()\n",
    "    color_mask = np.zeros_like(img)\n",
    "    color_mask[..., 0] = pred_mask / pred_mask.max()  # Red channel\n",
    "    overlay = 0.6 * img + 0.4 * color_mask\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Image\")\n",
    "    if gt_mask.ndim == 3 and gt_mask.shape[0] == 1:\n",
    "        gt_mask = gt_mask.squeeze(0)\n",
    "    axs[1].imshow(gt_mask, cmap='tab20')\n",
    "    axs[1].set_title(\"GT Mask\")\n",
    "    axs[2].imshow(pred_mask, cmap='tab20')\n",
    "    axs[2].set_title(\"Pred Mask\")\n",
    "    axs[3].imshow(overlay)\n",
    "    axs[3].set_title(\"Overlay\")\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e8349",
   "metadata": {},
   "source": [
    "### 5. Multi-Task Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28ab18",
   "metadata": {},
   "source": [
    "#### 5.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38349cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskDeepLab(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load DeepLabV3-ResNet50 with correct classifier shape\n",
    "        deeplab = deeplabv3_resnet50(weights=None, num_classes=num_classes)\n",
    "\n",
    "        # Shared backbone\n",
    "        self.backbone = deeplab.backbone      # dict: {\"out\": features}\n",
    "\n",
    "        # Shared ASPP segmentation head\n",
    "        self.segmentation_head = deeplab.classifier\n",
    "\n",
    "        # Classification head (global pooled backbone features)\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Backbone\n",
    "        features = self.backbone(x)[\"out\"]     # [B, 2048, H/32, W/32]\n",
    "\n",
    "        # 2. Classification head\n",
    "        pooled = F.adaptive_avg_pool2d(features, (1, 1)).flatten(1)\n",
    "        cls_logits = self.classifier(pooled)\n",
    "\n",
    "        # 3. Segmentation head\n",
    "        seg_logits = self.segmentation_head(features)\n",
    "        if isinstance(seg_logits, dict) and \"out\" in seg_logits:\n",
    "            seg_logits = seg_logits[\"out\"]\n",
    "        seg_logits = F.interpolate(seg_logits, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return cls_logits, seg_logits\n",
    "\n",
    "# Instantiate model\n",
    "model = MultiTaskDeepLab(num_classes).to(device)\n",
    "\n",
    "# Load DeepLab pretrained weights\n",
    "state = torch.load(\"models/segmentation_small_20.pth\", map_location=device)\n",
    "model.load_state_dict(state, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd01fa",
   "metadata": {},
   "source": [
    "#### 5.2 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "cls_criterion = nn.BCEWithLogitsLoss()\n",
    "seg_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10082a84",
   "metadata": {},
   "source": [
    "#### 5.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Model Losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Classification Metrics\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "subset_accs = []\n",
    "\n",
    "# Segmentation Metrics\n",
    "pixel_accs = []\n",
    "ious_list = []\n",
    "dice_list = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -----------------------------\n",
    "    # TRAINING\n",
    "    # -----------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\", leave=False):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        cls_targets = batch[\"ingredient_vector\"].to(device)\n",
    "        seg_targets = batch[\"label\"].to(device).squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cls_logits, seg_logits = model(images)\n",
    "\n",
    "        cls_loss = cls_criterion(cls_logits, cls_targets)\n",
    "        seg_loss = seg_criterion(seg_logits, seg_targets)\n",
    "        loss = cls_loss + seg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # VALIDATION\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    \n",
    "    all_cls_targets, all_cls_preds = [], []\n",
    "\n",
    "    epoch_pixel_acc = []\n",
    "    epoch_ious = []\n",
    "    epoch_dices = []\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Validation\", leave=False):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            cls_targets = batch[\"ingredient_vector\"].cpu().numpy()\n",
    "            seg_targets = batch[\"label\"].squeeze(1)\n",
    "            \n",
    "            cls_logits, seg_logits = model(images)\n",
    "            \n",
    "            # Classification\n",
    "            cls_probs = torch.sigmoid(cls_logits).cpu().numpy()\n",
    "            cls_preds = (cls_probs > 0.5).astype(int)\n",
    "            all_cls_targets.append(cls_targets)\n",
    "            all_cls_preds.append(cls_preds)\n",
    "            \n",
    "            # Segmentation metrics per batch\n",
    "            epoch_pixel_acc.append(pixel_accuracy(seg_logits, seg_targets))\n",
    "            epoch_ious.append(per_class_iou(seg_logits, seg_targets, num_classes))\n",
    "            epoch_dices.append(dice_score(seg_logits, seg_targets, num_classes))\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "    \n",
    "    train_losses.append(val_running_loss)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # Concatenate results\n",
    "    all_cls_targets = np.concatenate(all_cls_targets, axis=0)\n",
    "    all_cls_preds = np.concatenate(all_cls_preds, axis=0)\n",
    "    \n",
    "    all_cls_preds = (all_cls_preds > 0.5).astype(np.float32)\n",
    "\n",
    "    # Classification\n",
    "    precisions.append(precision_score(all_cls_targets, all_cls_preds, average='macro', zero_division=0))\n",
    "    recalls.append(recall_score(all_cls_targets, all_cls_preds, average='macro', zero_division=0))\n",
    "    f1_scores.append(f1_score(all_cls_targets, all_cls_preds, average='macro', zero_division=0))\n",
    "    subset_accs.append(np.mean(np.all(all_cls_targets == all_cls_preds, axis=1)))\n",
    "    \n",
    "    # Segmentation\n",
    "    pixel_accs.append(np.mean(epoch_pixel_acc))\n",
    "    ious_list.append(np.nanmean(np.array(epoch_ious), axis=0))\n",
    "    dice_list.append(np.nanmean(np.array(epoch_dices), axis=0))\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Joint Loss: {epoch_loss:.4f} | \"\n",
    "          f\"Val Joint Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        torch.save(model.state_dict(), \"models/multitask_best2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e63a0",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64b64c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Joint Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Classification metrics\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, precisions, label=\"Precision\")\n",
    "plt.plot(epochs, recalls, label=\"Recall\")\n",
    "plt.plot(epochs, f1_scores, label=\"F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Classification Metrics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, subset_accs, label=\"Subset Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Subset Accuracy\")\n",
    "plt.title(\"Subset Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Segmentation metrics\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, pixel_accs, label=\"Pixel Accuracy\")\n",
    "mean_ious = [np.nanmean(iou) for iou in ious_list]\n",
    "mean_dices = [np.nanmean(dice) for dice in dice_list]\n",
    "plt.plot(epochs, mean_ious, label=\"Mean IoU\")\n",
    "plt.plot(epochs, mean_dices, label=\"Mean Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Segmentation Metrics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88007a4e",
   "metadata": {},
   "source": [
    "#### 5.4 Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_display = 5\n",
    "model.eval()\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "images = batch[\"image\"].to(device)\n",
    "cls_targets = batch[\"ingredient_vector\"].cpu().numpy()\n",
    "seg_targets = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_logits, seg_logits = model(images)\n",
    "    cls_probs = torch.sigmoid(cls_logits).cpu().numpy()\n",
    "    cls_preds = (cls_probs > 0.5).astype(int)\n",
    "    seg_preds = torch.argmax(seg_logits, dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(num_display):\n",
    "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
    "    img = img.clip(0, 1)\n",
    "\n",
    "    true_cls_indices = np.where(cls_targets[i] == 1)[0]\n",
    "    pred_cls_indices = np.where(cls_preds[i] == 1)[0]\n",
    "    true_cls_labels = [label_list[k] for k in true_cls_indices]\n",
    "    pred_cls_labels = [label_list[k] for k in pred_cls_indices]\n",
    "\n",
    "    gt_mask = seg_targets[i]\n",
    "    pred_mask = seg_preds[i]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Image\")\n",
    "    axs[1].imshow(gt_mask, cmap='tab20')\n",
    "    axs[1].set_title(\"GT Mask\")\n",
    "    axs[2].imshow(pred_mask, cmap='tab20')\n",
    "    axs[2].set_title(\"Pred Mask\")\n",
    "    axs[3].imshow(img)\n",
    "    axs[3].set_title(f\"True: {true_cls_labels}\\nPred: {pred_cls_labels}\")\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
